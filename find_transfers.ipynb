{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ete3\n",
    "import re\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import igraph as ig\n",
    "import pickle as pkl\n",
    "\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from scipy.stats            import mannwhitneyu\n",
    "from collections            import Counter\n",
    "\n",
    "ncbi = ete3.NCBITaxa()\n",
    "%cd /work/eggNOG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_genomes = pd.read_csv('../kelsey/genomes.tab',\n",
    "                              sep='\\t',\n",
    "                              index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages = pd.DataFrame()\n",
    "for taxid in sampled_genomes.species_taxid.unique():\n",
    "    if pd.isna(taxid):\n",
    "        continue\n",
    "    lineages = lineages.append({tax_rank: tmp_taxid \n",
    "                                 for tmp_taxid, tax_rank in ncbi.get_rank(ncbi.get_lineage(taxid)).items()},\n",
    "                                ignore_index=True)\n",
    "lineages = lineages.reindex(columns=['class', 'family',  'genus', 'phylum',\n",
    "                                     'order', 'species', 'superkingdom']).copy()\n",
    "lineages = lineages.query('superkingdom == 2').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_groups  = pd.read_parquet('working_eggNOG_groups.parquet')\n",
    "working_trees   = pd.read_parquet('working_eggNOG_trees.parquet' )\n",
    "eggNOG_taxonomy = pd.read_parquet('eggNOG_taxonomy.parquet'      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_distances(group_id):\n",
    "    \n",
    "    tree = ete3.Tree(working_trees.loc[group_id, 'tree'])\n",
    "\n",
    "    leaf_names = []\n",
    "    for count, node in enumerate(tree.traverse()):\n",
    "        if node.is_leaf():\n",
    "            leaf_names.append(node.name)\n",
    "        else:\n",
    "            node.name = 'node_%i' % count\n",
    "    leaf_names = np.array(leaf_names)\n",
    "\n",
    "    nodes         = []\n",
    "    children      = []\n",
    "    branch_length = []\n",
    "    for node in tree.traverse():\n",
    "        if not node.is_leaf():\n",
    "            for child in node.get_children():\n",
    "                nodes.append(         node.name)\n",
    "                children.append(     child.name)\n",
    "                branch_length.append(child.dist)\n",
    "\n",
    "    branch_length_df                  = pd.DataFrame()\n",
    "    branch_length_df['node']          = nodes\n",
    "    branch_length_df['child']         = children\n",
    "    branch_length_df['branch_length'] = branch_length\n",
    "\n",
    "    dag  = ig.Graph.TupleList(edges=branch_length_df[['node', \n",
    "                                                      'child', \n",
    "                                                      'branch_length']].itertuples(index=False), \n",
    "                                directed=False, \n",
    "                                weights=True)\n",
    "    \n",
    "    dist_matrix = pd.DataFrame(index  =leaf_names, \n",
    "                               columns=leaf_names, \n",
    "                               data   =np.array(dag.shortest_paths(source=leaf_names, \n",
    "                                                                   target=leaf_names, \n",
    "                                                                   weights='weight'))\n",
    "                              )\n",
    "    return(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_taxa_graph(dist_matrix, phyla):\n",
    "    triu_indices       = np.triu_indices_from(dist_matrix, k=1)\n",
    "    \n",
    "    edge_list                 = pd.DataFrame()\n",
    "    edge_list['phylum1']      = phyla[triu_indices[0]]\n",
    "    edge_list['phylum2']      = phyla[triu_indices[1]]\n",
    "    edge_list['sequence1']    = dist_matrix.index[triu_indices[0]]\n",
    "    edge_list['sequence2']    = dist_matrix.index[triu_indices[1]]\n",
    "    edge_list['distance']     = dist_matrix.values[triu_indices]\n",
    "    edge_list['inverse_dist'] = np.e**np.negative(edge_list.distance)\n",
    "\n",
    "    graph  = ig.Graph.TupleList(edges=edge_list[['sequence1', \n",
    "                                                 'sequence2', \n",
    "                                                 'inverse_dist']].itertuples(index=False), \n",
    "                                directed=False, \n",
    "                                weights =True)\n",
    "    \n",
    "    return(edge_list, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cles(lessers, greaters):\n",
    "    #\n",
    "    # https://github.com/ajschumacher/cles/blob/master/cles.py\n",
    "    #\n",
    "    \"\"\"Common-Language Effect Size\n",
    "    Probability that a random draw from `greater` is in fact greater\n",
    "    than a random draw from `lesser`.\n",
    "    Args:\n",
    "      lesser, greater: Iterables of comparables.\n",
    "    \"\"\"\n",
    "    if len(lessers) == 0 and len(greaters) == 0:\n",
    "        raise ValueError('At least one argument must be non-empty')\n",
    "    # These values are a bit arbitrary, but make some sense.\n",
    "    # (It might be appropriate to warn for these cases.)\n",
    "    if len(lessers) == 0:\n",
    "        return 1\n",
    "    if len(greaters) == 0:\n",
    "        return 0\n",
    "    numerator = 0\n",
    "    lessers, greaters = sorted(lessers), sorted(greaters)\n",
    "    lesser_index = 0\n",
    "    for greater in greaters:\n",
    "        while lesser_index < len(lessers) and lessers[lesser_index] < greater:\n",
    "            lesser_index += 1\n",
    "        numerator += lesser_index  # the count less than the greater\n",
    "    denominator = len(lessers) * len(greaters)\n",
    "    return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_cluster(reference_phylum, minimal_freq_phyla, cluster_edges, cluster_nodes):\n",
    "\n",
    "    #\n",
    "    # store distances between reference phylum and others\n",
    "    cluster_dists = pd.DataFrame(columns=['phylum', 'median', 'distances'])\n",
    "\n",
    "    #\n",
    "    # traverse all phylum pairs containing the reference phylum\n",
    "    for phylum in minimal_freq_phyla:\n",
    "        if phylum == reference_phylum:\n",
    "            continue\n",
    "\n",
    "        inter_phyla = cluster_edges.loc[((cluster_edges.phylum1==reference_phylum) & (cluster_edges.phylum2==phylum)) |\\\n",
    "                                        ((cluster_edges.phylum2==reference_phylum) & (cluster_edges.phylum1==phylum))]\n",
    "        \n",
    "        #\n",
    "        # create a quadratic matrix of pairwise distances between phyla\n",
    "        #   rows and columns must be unique pairs of sequence names\n",
    "        indices     = np.unique(inter_phyla[['sequence1', 'sequence2']])\n",
    "        adjacencies = pd.DataFrame(index  =indices, \n",
    "                                   columns=indices,\n",
    "                                   data   =0.0)\n",
    "        \n",
    "        #\n",
    "        # add distances between sequences from the edge list to the quadratic matrix\n",
    "        #   as the matrix is quadratic, add values to both directions\n",
    "        indexer     = adjacencies.index.get_indexer\n",
    "        adjacencies.values[indexer(inter_phyla.sequence1), indexer(inter_phyla.sequence2)] = inter_phyla.distance.values\n",
    "        adjacencies.values[indexer(inter_phyla.sequence2), indexer(inter_phyla.sequence1)] = inter_phyla.distance.values\n",
    "\n",
    "        #\n",
    "        # sum the obtained distances into a single cell\n",
    "        tmp_closest_to_phylum = adjacencies.loc[cluster_nodes.loc[cluster_nodes.phylum==reference_phylum, 'name'],\n",
    "                                                cluster_nodes.loc[cluster_nodes.phylum==phylum,           'name']].sum()\n",
    "        tmp_closest_to_phylum.sort_values(inplace=True)\n",
    "        #\n",
    "        # and grab the five sequences from <phylum> closest to all from <reference phylum>\n",
    "        tmp_closest_to_phylum = tmp_closest_to_phylum.index[:5]\n",
    "\n",
    "        try:\n",
    "            #\n",
    "            # get all inter-phyla distances between\n",
    "            distances_to_reference_phylum = adjacencies.loc[\n",
    "                #\n",
    "                # all sequences from <reference phylum>\n",
    "                cluster_nodes.loc[cluster_nodes.phylum==reference_phylum, 'name'],\n",
    "                #\n",
    "                # the 5 sequences from <phylum> closest to <reference phylum>\n",
    "                tmp_closest_to_phylum\n",
    "            ].values.flatten()\n",
    "        except IndexError:\n",
    "            continue        \n",
    "\n",
    "        cluster_dists = cluster_dists.append(pd.Series(data =[phylum, \n",
    "                                                              np.median(distances_to_reference_phylum), \n",
    "                                                              distances_to_reference_phylum], \n",
    "                                                       index=['phylum', 'median', 'distances']),\n",
    "                                             ignore_index=True)\n",
    "    return(cluster_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phyla_evol_distances(group_id):    \n",
    "    dist_matrix = get_pairwise_distances(group_id)\n",
    "\n",
    "    taxids = [int(leaf.split('.')[0]) for leaf in dist_matrix.index]\n",
    "    phyla  = eggNOG_taxonomy.loc[taxids, 'phylum'].values.astype(int)\n",
    "\n",
    "    edge_list, graph  = create_taxa_graph(dist_matrix, phyla)\n",
    "\n",
    "    random.seed(12345)\n",
    "    clusters = graph.community_multilevel(weights='weight')\n",
    "\n",
    "    node_data = pd.DataFrame(columns=['name', 'phylum', 'cluster'],\n",
    "                             data   =zip(dist_matrix.index, \n",
    "                                         phyla, \n",
    "                                         clusters.membership)\n",
    "                            )\n",
    "    \n",
    "    cluster_evol_relations = {}\n",
    "    target_phyla = {1090,    # chlorobi\n",
    "                    1117,    # cyanobacteria\n",
    "                    1224,    # proteobacteria\n",
    "                    200795,  # chloroflexi\n",
    "                    976,     # bacteroidetes\n",
    "                    1134404, # ignavibacteriae\n",
    "                    1798710} # melainabacteria\n",
    "\n",
    "    for cluster_num in set(clusters.membership):\n",
    "        \n",
    "        cluster_nodes      = node_data[node_data.cluster==cluster_num]\n",
    "        minimal_freq_phyla = [phylum \n",
    "                              for phylum, frequency in Counter(cluster_nodes.phylum).items() \n",
    "                              if frequency>=5   # at least five sequences from a phylum\n",
    "                              and phylum > 0]   # given pandas manipulation, unknown phyla are represented \n",
    "                                                #   as NAN, which when forced to be int are negative numbers\n",
    "                                                #   that is why we ignore phylum taxids smaller than zero...\n",
    "       \n",
    "        #\n",
    "        # if there are fewer than two phyla of interested within the tree cluster there is no reason to\n",
    "        #   assess it...\n",
    "        if len( target_phyla.intersection( minimal_freq_phyla ) ) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_evol_relations[cluster_num] = {}\n",
    "        \n",
    "        #\n",
    "        # filter patristic distances from the whole tree to only those between sequences within the cluster\n",
    "        cluster_edges = edge_list.loc[(edge_list.sequence1.isin(cluster_nodes.name)) &\n",
    "                                      (edge_list.sequence2.isin(cluster_nodes.name)),\n",
    "                                      ['phylum1', 'phylum2', 'sequence1', 'sequence2', 'distance']]\n",
    "\n",
    "        #\n",
    "        # filter again, leaving only between sequences whose phylum fulfills the minimal frequency\n",
    "        cluster_edges      = cluster_edges[(cluster_edges.phylum1.isin(minimal_freq_phyla)) &\\\n",
    "                                           (cluster_edges.phylum2.isin(minimal_freq_phyla))]\n",
    "        #\n",
    "        # we will divide all pairwise distances by the cluster's mean\n",
    "        normalizer         = np.median(cluster_edges.distance)\n",
    "        #\n",
    "        # remove all intra-phylum distances\n",
    "        cluster_edges      = cluster_edges[cluster_edges.phylum1 != cluster_edges.phylum2] \n",
    "\n",
    "        #\n",
    "        # assess all inter-phyla distances, using each phylum as a reference to itself\n",
    "        #\n",
    "        for ref_phylum in target_phyla.intersection(minimal_freq_phyla):\n",
    "            cluster_dists = assess_cluster(ref_phylum, \n",
    "                                           minimal_freq_phyla, \n",
    "                                           cluster_edges,\n",
    "                                           cluster_nodes)\n",
    "\n",
    "            #\n",
    "            # sort phyla by its avg. distance to <reference phylum>\n",
    "            cluster_dists.sort_values('median', inplace=True)\n",
    "            cluster_evol_relations[cluster_num][ref_phylum] = {\n",
    "                'df'         :cluster_dists[['phylum', 'median']].copy(),\n",
    "                'significant':False\n",
    "            }\n",
    "            if not cluster_dists.shape[0]:\n",
    "                continue\n",
    "\n",
    "            cluster_evol_relations[cluster_num][ref_phylum]['df']['median']   /= normalizer\n",
    "            \n",
    "            #\n",
    "            # if there is only one <phylum> together with <reference phylum>, the proximity between\n",
    "            #   both is automaticaly significant\n",
    "            if cluster_dists.shape[0] == 1:\n",
    "                cluster_evol_relations[cluster_num][ref_phylum]['significant'] = True\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                #\n",
    "                # test if distances from the closest phylum to <reference phylum> is significantly\n",
    "                #   smaller than distances from the second closest phylum\n",
    "                hypothesis = mannwhitneyu(cluster_dists.iloc[0, 2], \n",
    "                                          cluster_dists.iloc[1, 2], \n",
    "                                          alternative='less')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            else:\n",
    "                #\n",
    "                # both cles lines below should work identically... I am leaving the above because it is the one I\n",
    "                #   used in the paper, no real reason...\n",
    "                effect_size = hypothesis.statistic / (len(cluster_dists.iloc[0, 2])*len(cluster_dists.iloc[1, 2]))\n",
    "#                 effect_size = 1-cles(cluster_dists.iloc[0, 2], cluster_dists.iloc[1, 2])\n",
    "\n",
    "                if hypothesis.pvalue < 0.01 and effect_size < 0.2:\n",
    "                    cluster_evol_relations[cluster_num][ref_phylum]['significant'] = True\n",
    "    \n",
    "    return(group_id, cluster_evol_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# get_phyla_evol_distances('COG0499')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool    = multiprocessing.Pool(processes=10, maxtasksperchild=5)\n",
    "results = pool.map_async(get_phyla_evol_distances, working_groups.group_id.values)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_results.pkl', 'wb') as out:\n",
    "    pkl.dump(results.get(), out)\n",
    "del(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
